{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8691a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2fec7",
   "metadata": {},
   "source": [
    "Firstly, the dataset is loaded and the features (X) and target (y) are extracted as arrays. Next, the data is split into 80% training and 20% testing to evaluate the model performance. Features are then scaled to have mean 0 and a standard deviation equal to 1, which helps models to converge faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "diabetes = fetch_openml(\"diabetes\",version=1,as_frame=True)\n",
    "X = diabetes.data.values\n",
    "y = diabetes.target.values\n",
    "X.shape\n",
    "y = (y=='tested_positive').astype(np.int64)\n",
    "\n",
    "# train test spliting\n",
    "test_size=0.2\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler=StandardScaler()\n",
    "Xtr= scaler.fit_transform(Xtr)\n",
    "Xte= scaler.transform(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d15a2a",
   "metadata": {},
   "source": [
    "In this part of the code, the MLP is defined with an input layer, four hidden layers and a last output layer. A dropout probability of 50% is also added as default: some neurons are randomly dropped during training to prevent overfitting. As specified in the forward definition, the activation functions used are ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887f2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1, dropout_prob=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.out = nn.Linear(64, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c919a",
   "metadata": {},
   "source": [
    "The next section specifies the hyperparameters of this MLP model. \n",
    "\n",
    "- epochs: The epochs are the number of times the training algorithm will iterate over the entire training dataset. Choosing a number too large might cause overfitting, while too small can result in underfitting. In this case, 100 were used. \n",
    "\n",
    "- learning rate: This hyperparameter controls how much the model weights are updated in response to the computed gradient during training. In this case, larger numbers can accelerate training but may cause divergence, while smaller values may slow down convergence. A learning rate of 0.0005 is used to ensure small, stable weight updates.\n",
    "\n",
    "- dropout: a value of 0.1 was chosen, meaning 10% of neurons are ignored at each training step\n",
    "\n",
    "- batch size: the batch size is the number of samples the model looks at before updating its weights. Smaller batch sizes can provide noisier gradient estimates, which may help the model generalize better. When a larger number is considered, it can provide more stable gradients, but requires more memory. In this case, a batch size of 64 was considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95aa28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=100\n",
    "lr=0.0005\n",
    "dropout=0.1\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9201e1",
   "metadata": {},
   "source": [
    "text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6e2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = torch.tensor(Xtr, dtype=torch.float32)\n",
    "ytr = torch.tensor(ytr, dtype=torch.float32)\n",
    "Xte = torch.tensor(Xte, dtype=torch.float32)\n",
    "yte = torch.tensor(yte, dtype=torch.float32)\n",
    "\n",
    "# Wrap Xtr and ytr into a dataset\n",
    "train_dataset = TensorDataset(Xtr, ytr)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44005e",
   "metadata": {},
   "source": [
    "text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82d9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Loss, Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP(input_size=Xtr.shape[1], dropout_prob=dropout).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # for binary classification\n",
    "criterion = nn.MSELoss() #for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce3097",
   "metadata": {},
   "source": [
    "text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ba86e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2565\n",
      "Epoch [2/100], Loss: 0.2301\n",
      "Epoch [3/100], Loss: 0.2116\n",
      "Epoch [4/100], Loss: 0.1987\n",
      "Epoch [5/100], Loss: 0.1819\n",
      "Epoch [6/100], Loss: 0.1726\n",
      "Epoch [7/100], Loss: 0.1696\n",
      "Epoch [8/100], Loss: 0.1623\n",
      "Epoch [9/100], Loss: 0.1524\n",
      "Epoch [10/100], Loss: 0.1585\n",
      "Epoch [11/100], Loss: 0.1564\n",
      "Epoch [12/100], Loss: 0.1468\n",
      "Epoch [13/100], Loss: 0.1478\n",
      "Epoch [14/100], Loss: 0.1513\n",
      "Epoch [15/100], Loss: 0.1469\n",
      "Epoch [16/100], Loss: 0.1503\n",
      "Epoch [17/100], Loss: 0.1494\n",
      "Epoch [18/100], Loss: 0.1438\n",
      "Epoch [19/100], Loss: 0.1448\n",
      "Epoch [20/100], Loss: 0.1439\n",
      "Epoch [21/100], Loss: 0.1422\n",
      "Epoch [22/100], Loss: 0.1406\n",
      "Epoch [23/100], Loss: 0.1404\n",
      "Epoch [24/100], Loss: 0.1412\n",
      "Epoch [25/100], Loss: 0.1429\n",
      "Epoch [26/100], Loss: 0.1404\n",
      "Epoch [27/100], Loss: 0.1388\n",
      "Epoch [28/100], Loss: 0.1400\n",
      "Epoch [29/100], Loss: 0.1346\n",
      "Epoch [30/100], Loss: 0.1348\n",
      "Epoch [31/100], Loss: 0.1339\n",
      "Epoch [32/100], Loss: 0.1396\n",
      "Epoch [33/100], Loss: 0.1342\n",
      "Epoch [34/100], Loss: 0.1394\n",
      "Epoch [35/100], Loss: 0.1317\n",
      "Epoch [36/100], Loss: 0.1385\n",
      "Epoch [37/100], Loss: 0.1339\n",
      "Epoch [38/100], Loss: 0.1294\n",
      "Epoch [39/100], Loss: 0.1356\n",
      "Epoch [40/100], Loss: 0.1328\n",
      "Epoch [41/100], Loss: 0.1347\n",
      "Epoch [42/100], Loss: 0.1312\n",
      "Epoch [43/100], Loss: 0.1315\n",
      "Epoch [44/100], Loss: 0.1335\n",
      "Epoch [45/100], Loss: 0.1275\n",
      "Epoch [46/100], Loss: 0.1278\n",
      "Epoch [47/100], Loss: 0.1306\n",
      "Epoch [48/100], Loss: 0.1270\n",
      "Epoch [49/100], Loss: 0.1322\n",
      "Epoch [50/100], Loss: 0.1260\n",
      "Epoch [51/100], Loss: 0.1316\n",
      "Epoch [52/100], Loss: 0.1236\n",
      "Epoch [53/100], Loss: 0.1242\n",
      "Epoch [54/100], Loss: 0.1264\n",
      "Epoch [55/100], Loss: 0.1291\n",
      "Epoch [56/100], Loss: 0.1283\n",
      "Epoch [57/100], Loss: 0.1271\n",
      "Epoch [58/100], Loss: 0.1276\n",
      "Epoch [59/100], Loss: 0.1293\n",
      "Epoch [60/100], Loss: 0.1241\n",
      "Epoch [61/100], Loss: 0.1205\n",
      "Epoch [62/100], Loss: 0.1254\n",
      "Epoch [63/100], Loss: 0.1180\n",
      "Epoch [64/100], Loss: 0.1197\n",
      "Epoch [65/100], Loss: 0.1193\n",
      "Epoch [66/100], Loss: 0.1228\n",
      "Epoch [67/100], Loss: 0.1204\n",
      "Epoch [68/100], Loss: 0.1166\n",
      "Epoch [69/100], Loss: 0.1222\n",
      "Epoch [70/100], Loss: 0.1167\n",
      "Epoch [71/100], Loss: 0.1131\n",
      "Epoch [72/100], Loss: 0.1139\n",
      "Epoch [73/100], Loss: 0.1183\n",
      "Epoch [74/100], Loss: 0.1171\n",
      "Epoch [75/100], Loss: 0.1152\n",
      "Epoch [76/100], Loss: 0.1101\n",
      "Epoch [77/100], Loss: 0.1123\n",
      "Epoch [78/100], Loss: 0.1112\n",
      "Epoch [79/100], Loss: 0.1151\n",
      "Epoch [80/100], Loss: 0.1120\n",
      "Epoch [81/100], Loss: 0.1175\n",
      "Epoch [82/100], Loss: 0.1146\n",
      "Epoch [83/100], Loss: 0.1055\n",
      "Epoch [84/100], Loss: 0.1110\n",
      "Epoch [85/100], Loss: 0.1103\n",
      "Epoch [86/100], Loss: 0.1036\n",
      "Epoch [87/100], Loss: 0.1072\n",
      "Epoch [88/100], Loss: 0.1080\n",
      "Epoch [89/100], Loss: 0.1071\n",
      "Epoch [90/100], Loss: 0.1131\n",
      "Epoch [91/100], Loss: 0.1113\n",
      "Epoch [92/100], Loss: 0.1117\n",
      "Epoch [93/100], Loss: 0.1095\n",
      "Epoch [94/100], Loss: 0.1033\n",
      "Epoch [95/100], Loss: 0.1122\n",
      "Epoch [96/100], Loss: 0.1086\n",
      "Epoch [97/100], Loss: 0.0951\n",
      "Epoch [98/100], Loss: 0.1146\n",
      "Epoch [99/100], Loss: 0.1022\n",
      "Epoch [100/100], Loss: 0.1029\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in train_dataloader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y.view(-1, 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2747ca",
   "metadata": {},
   "source": [
    "text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49bf64b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:0.22470130026340485\n"
     ]
    }
   ],
   "source": [
    "y_pred=model(Xte)\n",
    "#print(f'ACC:{accuracy_score(yte.detach().numpy(),y_pred.detach().numpy()>0.5)}') #classification\n",
    "print(f'MSE:{mean_squared_error(yte.detach().numpy(),y_pred.detach().numpy())}') #regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligent-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
