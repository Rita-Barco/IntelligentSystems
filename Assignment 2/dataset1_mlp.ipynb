{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba70c9c",
   "metadata": {},
   "source": [
    "# Dataset 1 - Regression task using MLP\n",
    "\n",
    "Link to my repo: https://github.com/Rita-Barco/IntelligentSystems.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8691a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d05dc",
   "metadata": {},
   "source": [
    "Firstly, the dataset is loaded and the features (X) and target (y) are extracted as arrays. Next, the data is split into 80% training and 20% testing to evaluate the model performance, specifying a random state so that the data division will be equal at each run. Features are then scaled to have mean 0 and a standard deviation equal to 1, which helps models to converge faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d7c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "diabetes = datasets.load_diabetes(as_frame=True)\n",
    "X = diabetes.data.values\n",
    "y = diabetes.target.values\n",
    "X.shape\n",
    "\n",
    "# train test spliting\n",
    "test_size=0.2\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler=StandardScaler()\n",
    "Xtr= scaler.fit_transform(Xtr)\n",
    "Xte= scaler.transform(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a7816",
   "metadata": {},
   "source": [
    "In this part of the code, the multilayer perceptron (MLP) is defined. It consists of an input layer, four fully connected hidden layers with 64 neurons each and a final output layer. After each hidden layer, a dropout operation is applied, with a default probability of 0.5. So, during training, half of the neurons are randomly deactivated to reduce overfitting and improve the network's ability to generalize. The hidden layers of this network use the ReLU (Rectified Linear Unit) activation function, which introduces non-linearity and enalbes the network to capture complex relationships in the data. On the other hand, the output layer has a single neuron and no activation function, leaving its interpretation and any task-specific processing to the choice of loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "887f2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1, dropout_prob=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.out = nn.Linear(64, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc30a8a",
   "metadata": {},
   "source": [
    "The next section specifies the hyperparameters of this MLP model. \n",
    "\n",
    "- epochs: The epochs are the number of times the training algorithm will iterate over the entire training dataset. Choosing a number too large might cause overfitting, while too small can result in underfitting. In this case, 100 were used. \n",
    "\n",
    "- learning rate: This hyperparameter controls how much the model weights are updated in response to the computed gradient during training. In this case, larger numbers can accelerate training but may cause divergence, while smaller values may slow down convergence. A learning rate of 0.0005 is used to ensure small, stable weight updates.\n",
    "\n",
    "- dropout: a value of 0.1 was chosen, meaning 10% of neurons are ignored at each training step\n",
    "\n",
    "- batch size: the batch size is the number of samples the model looks at before updating its weights. Smaller batch sizes can provide noisier gradient estimates, which may help the model generalize better. When a larger number is considered, it can provide more stable gradients, but requires more memory. In this case, a batch size of 64 was considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95aa28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=500\n",
    "lr=0.0005\n",
    "dropout=0.1\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0550ac6",
   "metadata": {},
   "source": [
    "Before training the model, the input features (Xtr, Xte) and targets (ytr, yte) are converted to PyTorch tensors. (Since PyTorch requires tensors for its computations and gradient tracking during training). Then, the training data is wrapped into a TensoDataset, which pairs each input sample with its corresponding target. This allows the data to be fed to the model in a structured way. Finally, a DataLoader is created from the TensorDataset. The DataLoader handles batching and shuffling: batching groups the training samples into batches of the specified batch_size (64) so that the model updates its weights after processing each batch, while shuffling randomly rearranges the data each epoch to improve generalization and prevent model from learning the order of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6e2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = torch.tensor(Xtr, dtype=torch.float32)\n",
    "ytr = torch.tensor(ytr, dtype=torch.float32)\n",
    "Xte = torch.tensor(Xte, dtype=torch.float32)\n",
    "yte = torch.tensor(yte, dtype=torch.float32)\n",
    "\n",
    "# Wrap Xtr and ytr into a dataset\n",
    "train_dataset = TensorDataset(Xtr, ytr)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a7721",
   "metadata": {},
   "source": [
    "In this next code snippet, the model is created and moved to the appropriate device (GPU if available, otherwise CPU). The MLP is initialized with the number of input features and the specified dropout probability.\n",
    "\n",
    "For this regression problem, the loss function (criterion) is set to MSEloss, which computes the mean squared error between the predicted and target values. The optimizer is Adam, which updates the model's weights during training using the gradients, with the specified learing rate (lr) controlling the size of these updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d82d9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Loss, Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP(input_size=Xtr.shape[1], dropout_prob=dropout).to(device)\n",
    "criterion = nn.MSELoss() #for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d14640",
   "metadata": {},
   "source": [
    "The model is trained for the specified number of epochs. At the beginning of each epoch, the model is set to training mode. For each batch from the DataLoader, the input features and targets are moved to the appropriate device. The model then performs a forward pass to compute the predictions (logits) and the loss is calulated using the chosen loss function. \n",
    "Before backpropagation, the optimizer's gradients are reset to zero. The loss is then backpropagated through the network (loss.backward()) and the optimizer updates the model's weights (optimizer.step()). The loss for each batch is accumulated and the average loss for the epoch is printed to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5ba86e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 30244.2539\n",
      "Epoch [2/500], Loss: 29602.6491\n",
      "Epoch [3/500], Loss: 29909.4281\n",
      "Epoch [4/500], Loss: 30052.2126\n",
      "Epoch [5/500], Loss: 29419.9691\n",
      "Epoch [6/500], Loss: 29260.0267\n",
      "Epoch [7/500], Loss: 29283.5358\n",
      "Epoch [8/500], Loss: 29097.3490\n",
      "Epoch [9/500], Loss: 28630.8831\n",
      "Epoch [10/500], Loss: 27632.1393\n",
      "Epoch [11/500], Loss: 27167.6370\n",
      "Epoch [12/500], Loss: 25582.7031\n",
      "Epoch [13/500], Loss: 24280.1396\n",
      "Epoch [14/500], Loss: 22311.8454\n",
      "Epoch [15/500], Loss: 18543.2939\n",
      "Epoch [16/500], Loss: 15084.8830\n",
      "Epoch [17/500], Loss: 11383.6195\n",
      "Epoch [18/500], Loss: 8110.3153\n",
      "Epoch [19/500], Loss: 6290.7570\n",
      "Epoch [20/500], Loss: 6381.3542\n",
      "Epoch [21/500], Loss: 6098.6797\n",
      "Epoch [22/500], Loss: 5517.1563\n",
      "Epoch [23/500], Loss: 5504.6137\n",
      "Epoch [24/500], Loss: 4793.9054\n",
      "Epoch [25/500], Loss: 4462.6587\n",
      "Epoch [26/500], Loss: 4360.7382\n",
      "Epoch [27/500], Loss: 4157.9683\n",
      "Epoch [28/500], Loss: 4267.4079\n",
      "Epoch [29/500], Loss: 4380.9024\n",
      "Epoch [30/500], Loss: 4268.3075\n",
      "Epoch [31/500], Loss: 3989.9591\n",
      "Epoch [32/500], Loss: 3918.5686\n",
      "Epoch [33/500], Loss: 3898.8246\n",
      "Epoch [34/500], Loss: 3853.6820\n",
      "Epoch [35/500], Loss: 3697.4995\n",
      "Epoch [36/500], Loss: 4058.7800\n",
      "Epoch [37/500], Loss: 3627.7293\n",
      "Epoch [38/500], Loss: 3399.2089\n",
      "Epoch [39/500], Loss: 3984.4694\n",
      "Epoch [40/500], Loss: 3634.7837\n",
      "Epoch [41/500], Loss: 3768.1523\n",
      "Epoch [42/500], Loss: 3510.1455\n",
      "Epoch [43/500], Loss: 3485.2113\n",
      "Epoch [44/500], Loss: 3706.3263\n",
      "Epoch [45/500], Loss: 3650.8492\n",
      "Epoch [46/500], Loss: 3705.1979\n",
      "Epoch [47/500], Loss: 3351.0583\n",
      "Epoch [48/500], Loss: 3645.6130\n",
      "Epoch [49/500], Loss: 3367.0911\n",
      "Epoch [50/500], Loss: 3587.0125\n",
      "Epoch [51/500], Loss: 3463.3720\n",
      "Epoch [52/500], Loss: 3385.4391\n",
      "Epoch [53/500], Loss: 3704.7754\n",
      "Epoch [54/500], Loss: 3262.8099\n",
      "Epoch [55/500], Loss: 3378.8096\n",
      "Epoch [56/500], Loss: 3423.2063\n",
      "Epoch [57/500], Loss: 3548.1030\n",
      "Epoch [58/500], Loss: 3413.4580\n",
      "Epoch [59/500], Loss: 3529.6572\n",
      "Epoch [60/500], Loss: 3358.7349\n",
      "Epoch [61/500], Loss: 3173.4073\n",
      "Epoch [62/500], Loss: 3217.0622\n",
      "Epoch [63/500], Loss: 3259.6888\n",
      "Epoch [64/500], Loss: 3494.2271\n",
      "Epoch [65/500], Loss: 3243.9827\n",
      "Epoch [66/500], Loss: 3384.9719\n",
      "Epoch [67/500], Loss: 3341.4890\n",
      "Epoch [68/500], Loss: 3129.6797\n",
      "Epoch [69/500], Loss: 3026.2764\n",
      "Epoch [70/500], Loss: 3394.3748\n",
      "Epoch [71/500], Loss: 3400.6237\n",
      "Epoch [72/500], Loss: 3408.7112\n",
      "Epoch [73/500], Loss: 3313.0278\n",
      "Epoch [74/500], Loss: 3142.9922\n",
      "Epoch [75/500], Loss: 3078.2019\n",
      "Epoch [76/500], Loss: 3398.1476\n",
      "Epoch [77/500], Loss: 3196.0121\n",
      "Epoch [78/500], Loss: 3109.6825\n",
      "Epoch [79/500], Loss: 3181.7117\n",
      "Epoch [80/500], Loss: 3400.4077\n",
      "Epoch [81/500], Loss: 3524.4045\n",
      "Epoch [82/500], Loss: 3341.5786\n",
      "Epoch [83/500], Loss: 3099.5171\n",
      "Epoch [84/500], Loss: 3311.9057\n",
      "Epoch [85/500], Loss: 3145.3064\n",
      "Epoch [86/500], Loss: 3126.6405\n",
      "Epoch [87/500], Loss: 3078.1215\n",
      "Epoch [88/500], Loss: 3187.8200\n",
      "Epoch [89/500], Loss: 3110.1781\n",
      "Epoch [90/500], Loss: 2905.0830\n",
      "Epoch [91/500], Loss: 2874.0490\n",
      "Epoch [92/500], Loss: 3100.3019\n",
      "Epoch [93/500], Loss: 3125.1375\n",
      "Epoch [94/500], Loss: 3080.5956\n",
      "Epoch [95/500], Loss: 3153.6757\n",
      "Epoch [96/500], Loss: 3130.9600\n",
      "Epoch [97/500], Loss: 2989.9389\n",
      "Epoch [98/500], Loss: 3090.5808\n",
      "Epoch [99/500], Loss: 2860.4014\n",
      "Epoch [100/500], Loss: 3119.6465\n",
      "Epoch [101/500], Loss: 3102.0424\n",
      "Epoch [102/500], Loss: 3210.3665\n",
      "Epoch [103/500], Loss: 3223.5560\n",
      "Epoch [104/500], Loss: 3025.3041\n",
      "Epoch [105/500], Loss: 2953.8171\n",
      "Epoch [106/500], Loss: 3155.8417\n",
      "Epoch [107/500], Loss: 3078.4412\n",
      "Epoch [108/500], Loss: 3337.6505\n",
      "Epoch [109/500], Loss: 3065.4097\n",
      "Epoch [110/500], Loss: 2903.4598\n",
      "Epoch [111/500], Loss: 3023.9173\n",
      "Epoch [112/500], Loss: 2971.4452\n",
      "Epoch [113/500], Loss: 3060.6220\n",
      "Epoch [114/500], Loss: 2972.5870\n",
      "Epoch [115/500], Loss: 3097.2747\n",
      "Epoch [116/500], Loss: 2958.0370\n",
      "Epoch [117/500], Loss: 2992.9158\n",
      "Epoch [118/500], Loss: 3050.5476\n",
      "Epoch [119/500], Loss: 3074.8172\n",
      "Epoch [120/500], Loss: 3035.0404\n",
      "Epoch [121/500], Loss: 3176.8413\n",
      "Epoch [122/500], Loss: 2932.9336\n",
      "Epoch [123/500], Loss: 2887.1700\n",
      "Epoch [124/500], Loss: 2956.2575\n",
      "Epoch [125/500], Loss: 2740.9408\n",
      "Epoch [126/500], Loss: 3064.7135\n",
      "Epoch [127/500], Loss: 3117.3131\n",
      "Epoch [128/500], Loss: 3130.0173\n",
      "Epoch [129/500], Loss: 3009.8529\n",
      "Epoch [130/500], Loss: 3156.9705\n",
      "Epoch [131/500], Loss: 2985.1277\n",
      "Epoch [132/500], Loss: 2964.7298\n",
      "Epoch [133/500], Loss: 2962.7710\n",
      "Epoch [134/500], Loss: 2878.3938\n",
      "Epoch [135/500], Loss: 3036.2184\n",
      "Epoch [136/500], Loss: 2849.1039\n",
      "Epoch [137/500], Loss: 2999.5713\n",
      "Epoch [138/500], Loss: 2914.0251\n",
      "Epoch [139/500], Loss: 2933.4342\n",
      "Epoch [140/500], Loss: 2927.8320\n",
      "Epoch [141/500], Loss: 2988.8789\n",
      "Epoch [142/500], Loss: 3061.2783\n",
      "Epoch [143/500], Loss: 3147.5051\n",
      "Epoch [144/500], Loss: 3054.6100\n",
      "Epoch [145/500], Loss: 3037.9634\n",
      "Epoch [146/500], Loss: 3083.4925\n",
      "Epoch [147/500], Loss: 2779.9758\n",
      "Epoch [148/500], Loss: 2801.4561\n",
      "Epoch [149/500], Loss: 3119.5695\n",
      "Epoch [150/500], Loss: 2747.2363\n",
      "Epoch [151/500], Loss: 3126.3380\n",
      "Epoch [152/500], Loss: 2951.3748\n",
      "Epoch [153/500], Loss: 2899.1875\n",
      "Epoch [154/500], Loss: 2736.2657\n",
      "Epoch [155/500], Loss: 2595.0502\n",
      "Epoch [156/500], Loss: 2874.4813\n",
      "Epoch [157/500], Loss: 2814.9808\n",
      "Epoch [158/500], Loss: 2985.1157\n",
      "Epoch [159/500], Loss: 3125.0569\n",
      "Epoch [160/500], Loss: 2966.0409\n",
      "Epoch [161/500], Loss: 2810.8494\n",
      "Epoch [162/500], Loss: 2960.3315\n",
      "Epoch [163/500], Loss: 3108.7457\n",
      "Epoch [164/500], Loss: 3086.1528\n",
      "Epoch [165/500], Loss: 3074.6053\n",
      "Epoch [166/500], Loss: 3027.4970\n",
      "Epoch [167/500], Loss: 3133.7642\n",
      "Epoch [168/500], Loss: 3102.7133\n",
      "Epoch [169/500], Loss: 2933.3355\n",
      "Epoch [170/500], Loss: 2958.5912\n",
      "Epoch [171/500], Loss: 2674.1257\n",
      "Epoch [172/500], Loss: 3091.0349\n",
      "Epoch [173/500], Loss: 2932.4464\n",
      "Epoch [174/500], Loss: 3027.1186\n",
      "Epoch [175/500], Loss: 2961.1648\n",
      "Epoch [176/500], Loss: 3069.5557\n",
      "Epoch [177/500], Loss: 2810.9796\n",
      "Epoch [178/500], Loss: 2936.6204\n",
      "Epoch [179/500], Loss: 3057.2380\n",
      "Epoch [180/500], Loss: 2894.3113\n",
      "Epoch [181/500], Loss: 2902.8765\n",
      "Epoch [182/500], Loss: 2685.1396\n",
      "Epoch [183/500], Loss: 2879.4230\n",
      "Epoch [184/500], Loss: 2807.9706\n",
      "Epoch [185/500], Loss: 2987.2776\n",
      "Epoch [186/500], Loss: 2844.6029\n",
      "Epoch [187/500], Loss: 2877.6811\n",
      "Epoch [188/500], Loss: 2985.4235\n",
      "Epoch [189/500], Loss: 2873.1045\n",
      "Epoch [190/500], Loss: 2835.5298\n",
      "Epoch [191/500], Loss: 3075.5296\n",
      "Epoch [192/500], Loss: 2924.7374\n",
      "Epoch [193/500], Loss: 2838.8458\n",
      "Epoch [194/500], Loss: 2934.2585\n",
      "Epoch [195/500], Loss: 2860.4685\n",
      "Epoch [196/500], Loss: 2716.8066\n",
      "Epoch [197/500], Loss: 2911.1321\n",
      "Epoch [198/500], Loss: 2842.8221\n",
      "Epoch [199/500], Loss: 2883.0878\n",
      "Epoch [200/500], Loss: 2867.2894\n",
      "Epoch [201/500], Loss: 2639.4535\n",
      "Epoch [202/500], Loss: 2827.1584\n",
      "Epoch [203/500], Loss: 2761.3584\n",
      "Epoch [204/500], Loss: 2742.3201\n",
      "Epoch [205/500], Loss: 2920.5617\n",
      "Epoch [206/500], Loss: 2651.0258\n",
      "Epoch [207/500], Loss: 3034.9513\n",
      "Epoch [208/500], Loss: 2927.5756\n",
      "Epoch [209/500], Loss: 3017.3129\n",
      "Epoch [210/500], Loss: 2773.6410\n",
      "Epoch [211/500], Loss: 2608.9196\n",
      "Epoch [212/500], Loss: 2838.7762\n",
      "Epoch [213/500], Loss: 2606.4563\n",
      "Epoch [214/500], Loss: 2593.0901\n",
      "Epoch [215/500], Loss: 2764.3976\n",
      "Epoch [216/500], Loss: 2806.5282\n",
      "Epoch [217/500], Loss: 2908.5213\n",
      "Epoch [218/500], Loss: 2948.1766\n",
      "Epoch [219/500], Loss: 2897.3961\n",
      "Epoch [220/500], Loss: 2871.8215\n",
      "Epoch [221/500], Loss: 2870.8611\n",
      "Epoch [222/500], Loss: 2528.5749\n",
      "Epoch [223/500], Loss: 2809.1978\n",
      "Epoch [224/500], Loss: 3129.8654\n",
      "Epoch [225/500], Loss: 2795.3337\n",
      "Epoch [226/500], Loss: 2806.1368\n",
      "Epoch [227/500], Loss: 2812.9211\n",
      "Epoch [228/500], Loss: 2845.7528\n",
      "Epoch [229/500], Loss: 3036.1665\n",
      "Epoch [230/500], Loss: 3010.1884\n",
      "Epoch [231/500], Loss: 2867.5798\n",
      "Epoch [232/500], Loss: 2993.6356\n",
      "Epoch [233/500], Loss: 2818.6527\n",
      "Epoch [234/500], Loss: 2806.6183\n",
      "Epoch [235/500], Loss: 2795.0024\n",
      "Epoch [236/500], Loss: 2868.2631\n",
      "Epoch [237/500], Loss: 2894.0216\n",
      "Epoch [238/500], Loss: 2895.0227\n",
      "Epoch [239/500], Loss: 2832.1404\n",
      "Epoch [240/500], Loss: 2692.0204\n",
      "Epoch [241/500], Loss: 2962.5718\n",
      "Epoch [242/500], Loss: 2670.7833\n",
      "Epoch [243/500], Loss: 2825.7085\n",
      "Epoch [244/500], Loss: 3042.8764\n",
      "Epoch [245/500], Loss: 2681.0640\n",
      "Epoch [246/500], Loss: 2797.8663\n",
      "Epoch [247/500], Loss: 2689.8411\n",
      "Epoch [248/500], Loss: 2639.7794\n",
      "Epoch [249/500], Loss: 2943.4769\n",
      "Epoch [250/500], Loss: 2673.7614\n",
      "Epoch [251/500], Loss: 2741.2965\n",
      "Epoch [252/500], Loss: 2883.7150\n",
      "Epoch [253/500], Loss: 2784.9427\n",
      "Epoch [254/500], Loss: 3131.6409\n",
      "Epoch [255/500], Loss: 2845.2305\n",
      "Epoch [256/500], Loss: 2834.1233\n",
      "Epoch [257/500], Loss: 2978.9095\n",
      "Epoch [258/500], Loss: 2800.6211\n",
      "Epoch [259/500], Loss: 2824.8452\n",
      "Epoch [260/500], Loss: 2832.6305\n",
      "Epoch [261/500], Loss: 2961.7201\n",
      "Epoch [262/500], Loss: 2686.2277\n",
      "Epoch [263/500], Loss: 2832.2459\n",
      "Epoch [264/500], Loss: 2807.6583\n",
      "Epoch [265/500], Loss: 2759.3759\n",
      "Epoch [266/500], Loss: 2836.2866\n",
      "Epoch [267/500], Loss: 2829.0108\n",
      "Epoch [268/500], Loss: 2953.6956\n",
      "Epoch [269/500], Loss: 2734.0639\n",
      "Epoch [270/500], Loss: 2685.6023\n",
      "Epoch [271/500], Loss: 2881.3980\n",
      "Epoch [272/500], Loss: 2875.3766\n",
      "Epoch [273/500], Loss: 2878.0979\n",
      "Epoch [274/500], Loss: 2848.6876\n",
      "Epoch [275/500], Loss: 2815.5710\n",
      "Epoch [276/500], Loss: 2677.2764\n",
      "Epoch [277/500], Loss: 2813.9400\n",
      "Epoch [278/500], Loss: 2670.6047\n",
      "Epoch [279/500], Loss: 2735.7365\n",
      "Epoch [280/500], Loss: 2585.3462\n",
      "Epoch [281/500], Loss: 2664.0067\n",
      "Epoch [282/500], Loss: 2783.4580\n",
      "Epoch [283/500], Loss: 2803.6425\n",
      "Epoch [284/500], Loss: 2533.5844\n",
      "Epoch [285/500], Loss: 2930.4538\n",
      "Epoch [286/500], Loss: 2682.2250\n",
      "Epoch [287/500], Loss: 2704.8036\n",
      "Epoch [288/500], Loss: 2574.1664\n",
      "Epoch [289/500], Loss: 2742.5839\n",
      "Epoch [290/500], Loss: 2666.7096\n",
      "Epoch [291/500], Loss: 2661.2277\n",
      "Epoch [292/500], Loss: 2686.9058\n",
      "Epoch [293/500], Loss: 2787.6667\n",
      "Epoch [294/500], Loss: 2665.7055\n",
      "Epoch [295/500], Loss: 2773.3392\n",
      "Epoch [296/500], Loss: 2882.8649\n",
      "Epoch [297/500], Loss: 2679.8104\n",
      "Epoch [298/500], Loss: 2875.4068\n",
      "Epoch [299/500], Loss: 2890.3697\n",
      "Epoch [300/500], Loss: 2786.9682\n",
      "Epoch [301/500], Loss: 2874.6059\n",
      "Epoch [302/500], Loss: 2461.6878\n",
      "Epoch [303/500], Loss: 2847.9742\n",
      "Epoch [304/500], Loss: 2809.1416\n",
      "Epoch [305/500], Loss: 2878.6184\n",
      "Epoch [306/500], Loss: 2627.5478\n",
      "Epoch [307/500], Loss: 2834.5046\n",
      "Epoch [308/500], Loss: 2701.4702\n",
      "Epoch [309/500], Loss: 2587.6391\n",
      "Epoch [310/500], Loss: 2819.9811\n",
      "Epoch [311/500], Loss: 2583.8981\n",
      "Epoch [312/500], Loss: 2764.8792\n",
      "Epoch [313/500], Loss: 2904.2271\n",
      "Epoch [314/500], Loss: 2881.3574\n",
      "Epoch [315/500], Loss: 2888.9448\n",
      "Epoch [316/500], Loss: 2664.3125\n",
      "Epoch [317/500], Loss: 2712.5819\n",
      "Epoch [318/500], Loss: 2872.5009\n",
      "Epoch [319/500], Loss: 2870.1528\n",
      "Epoch [320/500], Loss: 2679.1171\n",
      "Epoch [321/500], Loss: 2682.3353\n",
      "Epoch [322/500], Loss: 2636.7689\n",
      "Epoch [323/500], Loss: 3077.2704\n",
      "Epoch [324/500], Loss: 2934.6869\n",
      "Epoch [325/500], Loss: 2907.7829\n",
      "Epoch [326/500], Loss: 2803.7036\n",
      "Epoch [327/500], Loss: 2693.3280\n",
      "Epoch [328/500], Loss: 2635.1486\n",
      "Epoch [329/500], Loss: 2612.0952\n",
      "Epoch [330/500], Loss: 2583.9495\n",
      "Epoch [331/500], Loss: 2750.9206\n",
      "Epoch [332/500], Loss: 2494.6383\n",
      "Epoch [333/500], Loss: 2562.8142\n",
      "Epoch [334/500], Loss: 2588.6215\n",
      "Epoch [335/500], Loss: 2656.8618\n",
      "Epoch [336/500], Loss: 2748.0598\n",
      "Epoch [337/500], Loss: 2777.5869\n",
      "Epoch [338/500], Loss: 2620.6820\n",
      "Epoch [339/500], Loss: 2677.2662\n",
      "Epoch [340/500], Loss: 2549.4153\n",
      "Epoch [341/500], Loss: 2481.4909\n",
      "Epoch [342/500], Loss: 2516.1365\n",
      "Epoch [343/500], Loss: 2583.2065\n",
      "Epoch [344/500], Loss: 2863.2138\n",
      "Epoch [345/500], Loss: 2566.5123\n",
      "Epoch [346/500], Loss: 2769.9626\n",
      "Epoch [347/500], Loss: 2940.7745\n",
      "Epoch [348/500], Loss: 2561.0885\n",
      "Epoch [349/500], Loss: 2800.8539\n",
      "Epoch [350/500], Loss: 2652.3498\n",
      "Epoch [351/500], Loss: 2562.9125\n",
      "Epoch [352/500], Loss: 2788.2217\n",
      "Epoch [353/500], Loss: 2609.3795\n",
      "Epoch [354/500], Loss: 2726.6859\n",
      "Epoch [355/500], Loss: 2644.5959\n",
      "Epoch [356/500], Loss: 2572.5606\n",
      "Epoch [357/500], Loss: 2533.2662\n",
      "Epoch [358/500], Loss: 2610.7431\n",
      "Epoch [359/500], Loss: 2729.9602\n",
      "Epoch [360/500], Loss: 2763.0143\n",
      "Epoch [361/500], Loss: 2660.2589\n",
      "Epoch [362/500], Loss: 2591.2961\n",
      "Epoch [363/500], Loss: 2645.8312\n",
      "Epoch [364/500], Loss: 2566.2149\n",
      "Epoch [365/500], Loss: 2598.3697\n",
      "Epoch [366/500], Loss: 2738.2279\n",
      "Epoch [367/500], Loss: 2659.8821\n",
      "Epoch [368/500], Loss: 2793.6280\n",
      "Epoch [369/500], Loss: 2554.1853\n",
      "Epoch [370/500], Loss: 2773.2965\n",
      "Epoch [371/500], Loss: 2750.3432\n",
      "Epoch [372/500], Loss: 2662.9353\n",
      "Epoch [373/500], Loss: 2584.3648\n",
      "Epoch [374/500], Loss: 2666.6028\n",
      "Epoch [375/500], Loss: 2942.2079\n",
      "Epoch [376/500], Loss: 2740.9792\n",
      "Epoch [377/500], Loss: 2700.9462\n",
      "Epoch [378/500], Loss: 2873.5090\n",
      "Epoch [379/500], Loss: 2609.6099\n",
      "Epoch [380/500], Loss: 2594.8564\n",
      "Epoch [381/500], Loss: 2649.9842\n",
      "Epoch [382/500], Loss: 2643.2678\n",
      "Epoch [383/500], Loss: 2656.6103\n",
      "Epoch [384/500], Loss: 2677.6730\n",
      "Epoch [385/500], Loss: 2573.9687\n",
      "Epoch [386/500], Loss: 2655.0438\n",
      "Epoch [387/500], Loss: 2537.1407\n",
      "Epoch [388/500], Loss: 2556.1333\n",
      "Epoch [389/500], Loss: 2710.3426\n",
      "Epoch [390/500], Loss: 2642.5328\n",
      "Epoch [391/500], Loss: 2865.0409\n",
      "Epoch [392/500], Loss: 2651.3333\n",
      "Epoch [393/500], Loss: 2644.8333\n",
      "Epoch [394/500], Loss: 2663.1386\n",
      "Epoch [395/500], Loss: 2740.2094\n",
      "Epoch [396/500], Loss: 2811.3168\n",
      "Epoch [397/500], Loss: 2575.6952\n",
      "Epoch [398/500], Loss: 2588.1061\n",
      "Epoch [399/500], Loss: 2767.1466\n",
      "Epoch [400/500], Loss: 2669.0881\n",
      "Epoch [401/500], Loss: 2514.5167\n",
      "Epoch [402/500], Loss: 2747.1106\n",
      "Epoch [403/500], Loss: 2584.1116\n",
      "Epoch [404/500], Loss: 2600.0354\n",
      "Epoch [405/500], Loss: 2666.9398\n",
      "Epoch [406/500], Loss: 2679.4450\n",
      "Epoch [407/500], Loss: 2682.1688\n",
      "Epoch [408/500], Loss: 2591.0333\n",
      "Epoch [409/500], Loss: 2615.7495\n",
      "Epoch [410/500], Loss: 2403.4923\n",
      "Epoch [411/500], Loss: 2493.0406\n",
      "Epoch [412/500], Loss: 2658.2291\n",
      "Epoch [413/500], Loss: 2591.8474\n",
      "Epoch [414/500], Loss: 2686.8649\n",
      "Epoch [415/500], Loss: 2604.7883\n",
      "Epoch [416/500], Loss: 2547.3173\n",
      "Epoch [417/500], Loss: 2633.5964\n",
      "Epoch [418/500], Loss: 2628.1758\n",
      "Epoch [419/500], Loss: 2570.2413\n",
      "Epoch [420/500], Loss: 2558.5911\n",
      "Epoch [421/500], Loss: 2509.9022\n",
      "Epoch [422/500], Loss: 2383.8522\n",
      "Epoch [423/500], Loss: 2825.7221\n",
      "Epoch [424/500], Loss: 2738.6297\n",
      "Epoch [425/500], Loss: 2439.5175\n",
      "Epoch [426/500], Loss: 2440.1983\n",
      "Epoch [427/500], Loss: 2634.6629\n",
      "Epoch [428/500], Loss: 2634.4823\n",
      "Epoch [429/500], Loss: 2476.1345\n",
      "Epoch [430/500], Loss: 2619.9914\n",
      "Epoch [431/500], Loss: 2482.8231\n",
      "Epoch [432/500], Loss: 2451.8642\n",
      "Epoch [433/500], Loss: 2497.1937\n",
      "Epoch [434/500], Loss: 2404.6583\n",
      "Epoch [435/500], Loss: 2786.3989\n",
      "Epoch [436/500], Loss: 2670.3346\n",
      "Epoch [437/500], Loss: 2700.3441\n",
      "Epoch [438/500], Loss: 2743.7545\n",
      "Epoch [439/500], Loss: 2390.9974\n",
      "Epoch [440/500], Loss: 2570.7304\n",
      "Epoch [441/500], Loss: 2521.7540\n",
      "Epoch [442/500], Loss: 2734.5043\n",
      "Epoch [443/500], Loss: 2527.6887\n",
      "Epoch [444/500], Loss: 2637.6180\n",
      "Epoch [445/500], Loss: 2709.0972\n",
      "Epoch [446/500], Loss: 2664.5741\n",
      "Epoch [447/500], Loss: 2713.5393\n",
      "Epoch [448/500], Loss: 2616.0883\n",
      "Epoch [449/500], Loss: 2532.7775\n",
      "Epoch [450/500], Loss: 2350.1550\n",
      "Epoch [451/500], Loss: 2599.4117\n",
      "Epoch [452/500], Loss: 2756.5025\n",
      "Epoch [453/500], Loss: 2580.7146\n",
      "Epoch [454/500], Loss: 2727.3437\n",
      "Epoch [455/500], Loss: 2763.4895\n",
      "Epoch [456/500], Loss: 2557.2891\n",
      "Epoch [457/500], Loss: 2687.0887\n",
      "Epoch [458/500], Loss: 2730.3819\n",
      "Epoch [459/500], Loss: 2585.6844\n",
      "Epoch [460/500], Loss: 2720.6953\n",
      "Epoch [461/500], Loss: 2574.1555\n",
      "Epoch [462/500], Loss: 2711.8068\n",
      "Epoch [463/500], Loss: 2455.9809\n",
      "Epoch [464/500], Loss: 2502.6475\n",
      "Epoch [465/500], Loss: 2672.0043\n",
      "Epoch [466/500], Loss: 2371.9433\n",
      "Epoch [467/500], Loss: 2308.1161\n",
      "Epoch [468/500], Loss: 2615.0562\n",
      "Epoch [469/500], Loss: 2610.1820\n",
      "Epoch [470/500], Loss: 2645.0161\n",
      "Epoch [471/500], Loss: 2790.5447\n",
      "Epoch [472/500], Loss: 2450.3065\n",
      "Epoch [473/500], Loss: 2449.4297\n",
      "Epoch [474/500], Loss: 2791.2896\n",
      "Epoch [475/500], Loss: 2508.9796\n",
      "Epoch [476/500], Loss: 2274.2989\n",
      "Epoch [477/500], Loss: 2815.3691\n",
      "Epoch [478/500], Loss: 2629.8772\n",
      "Epoch [479/500], Loss: 2352.3334\n",
      "Epoch [480/500], Loss: 2328.9055\n",
      "Epoch [481/500], Loss: 2531.0608\n",
      "Epoch [482/500], Loss: 2589.4650\n",
      "Epoch [483/500], Loss: 2591.8410\n",
      "Epoch [484/500], Loss: 2575.9783\n",
      "Epoch [485/500], Loss: 2747.3016\n",
      "Epoch [486/500], Loss: 2603.6646\n",
      "Epoch [487/500], Loss: 2508.6507\n",
      "Epoch [488/500], Loss: 2616.4672\n",
      "Epoch [489/500], Loss: 2626.8855\n",
      "Epoch [490/500], Loss: 2433.9315\n",
      "Epoch [491/500], Loss: 2651.8472\n",
      "Epoch [492/500], Loss: 2615.7287\n",
      "Epoch [493/500], Loss: 2639.9282\n",
      "Epoch [494/500], Loss: 2523.4018\n",
      "Epoch [495/500], Loss: 2372.5251\n",
      "Epoch [496/500], Loss: 2708.0761\n",
      "Epoch [497/500], Loss: 2455.6845\n",
      "Epoch [498/500], Loss: 2310.7953\n",
      "Epoch [499/500], Loss: 2544.8973\n",
      "Epoch [500/500], Loss: 2524.7968\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in train_dataloader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y.view(-1, 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d9cbc",
   "metadata": {},
   "source": [
    "Finally, after training, the model is evaluated on the test set. The test features (Xte) are passeed through the trained model to obtain predictions (y_pred).\n",
    "\n",
    "The predicted values are compared with the true targets (yte) to calculate the mean squared error (MSE), which measures the average squared difference between predicted and actual values. The model achieved an MSE of, approximately, 3041.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49bf64b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:3041.70654296875\n"
     ]
    }
   ],
   "source": [
    "y_pred=model(Xte)\n",
    "print(f'MSE:{mean_squared_error(yte.detach().numpy(),y_pred.detach().numpy())}') #regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligent-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
